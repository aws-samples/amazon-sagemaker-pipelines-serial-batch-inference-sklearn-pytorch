{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66fcbeb-4291-4028-9656-1f95c52d248e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sagemaker Pipelines SKLearn-->PyTorch Serial Batch Inference Demo End to End\n",
    "\n",
    "This notebook demonstrates the end to end process of creating a Sagemaker Pipeline for data preprocessing with scikit-learn and model training with PyTorch, registering the pipeline model, and running serial batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ea61d-fcb9-403c-b8c6-dd64977cb377",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "1. [Configure AWS](#configure)\n",
    "2. [Load Data](#data)\n",
    "3. [Define Pipeline Parameters](#params)\n",
    "4. [Define a `features.py` script for training and inference](#features)\n",
    "5. [Define a `model.py` script for training and inference](#model)\n",
    "6. [Define model creation and registration steps](#createregister)\n",
    "7. [Configure Sagemaker Pipeline](#pipeline)\n",
    "8. [Execute Sagemaker Pipeline to build features, train model, register artifacts, and create Sagemaker Model](#submit)\n",
    "9. [Pass pipeline to Batch Transform for serial inference](#inference)\n",
    "10. [Retrieve output batch inference data](#batch_inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64477410-15de-45e4-b143-5d892f8a5eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  !pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd72f2e-027c-424c-8ac4-efe210e234b8",
   "metadata": {},
   "source": [
    "### 1. Configure AWS <a name=\"configure\"></a><a name=\"configure\"></a>\n",
    "\n",
    "Set up your Sagemaker Session, Sagemaker Pipeline session, roles, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a43688-4330-4e1c-b8c8-46b28f264a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "{'DEFAULT_BUCKET': 'pytorch-serial-inference-demo', 'MODEL_PACKAGE_GROUP_NAME': 'PipelinePyTorchPackageGroup', 'PREFIX': 'pipeline-pytorch-example', 'PIPELINE_NAME': 'serial-pytorch-pipeline', 'INPUT_DATA': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/housing_data/raw', 'MODEL_APPROVAL_STATUS': 'Approved', 'PROCESSING_INSTANCE_TYPE': 'ml.m5.xlarge', 'PROCESSING_INSTANCE_COUNT': 1, 'TRAINING_INSTANCE_TYPE': 'ml.m5.xlarge', 'CREATE_MODEL_INSTANCE_TYPE': 'ml.m5.large', 'BATCH_TRANSFORM_INSTANCE_COUNT': 1, 'BATCH_TRANSFORM_INSTANCE_TYPE': 'ml.m4.xlarge'}\n",
      "Region: ca-central-1\n",
      "Role: arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063\n",
      "Bucket: pytorch-serial-inference-demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Load configs\n",
    "with open('config.json', 'r') as file:\n",
    "    config_data = json.load(file)\n",
    "print(\"Configs:\")    \n",
    "print(config_data)\n",
    "\n",
    "# Configure boto3, bucket info, and Sagemaker Sessions\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(default_bucket = config_data['DEFAULT_BUCKET'], boto_session=sess)\n",
    "pipeline_session = PipelineSession(default_bucket = config_data['DEFAULT_BUCKET'], boto_session=sess) \n",
    "\n",
    "# Configure region\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41efae8f-7683-40ab-8eca-751c3bfe60d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define prefixes and names\n",
    "model_package_group_name = config_data['MODEL_PACKAGE_GROUP_NAME']\n",
    "prefix = config_data['PREFIX']\n",
    "pipeline_name = config_data['PIPELINE_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9b9a3-b80f-4467-9f89-3562c02d3148",
   "metadata": {},
   "source": [
    "### 2. Load Dataset to Studio & Upload to S3 for training <a name=\"data\"></a><a name=\"data\"></a>\n",
    "\n",
    "We use the California housing dataset.\n",
    "\n",
    "More info on the dataset:\n",
    "* This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/\n",
    "* The target variable is the median house value for California districts.\n",
    "* This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed5d05d-c61e-49f8-bd6f-a6c28361e092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"housing_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), \"housing_data/raw\")\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a911ed-f74a-41dd-b85e-6672a6423d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/california_housing/cal_housing.tgz\",\n",
    "    \"cal_housing.tgz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cddfe44-b7b3-4269-98f1-27dc12435c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: CaliforniaHousing/cal_housing.data: Cannot change ownership to uid 10017, gid 166: Operation not permitted\n",
      "tar: CaliforniaHousing/cal_housing.domain: Cannot change ownership to uid 10017, gid 166: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!tar -zxf cal_housing.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70286ee8-166b-40c7-a0ac-e092085b4701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://pytorch-serial-inference-demo/pipeline-pytorch-example/housing_data/raw\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "cal_housing_df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)\n",
    "cal_housing_df[\n",
    "    \"medianHouseValue\"\n",
    "] /= 500000  # Scaling target down to avoid overcomplicating the example\n",
    "cal_housing_df.to_csv(f\"./housing_data/raw/raw_data_all.csv\", header=True, index=False)\n",
    "rawdata_s3_prefix = \"{}/housing_data/raw\".format(prefix)\n",
    "raw_s3 = sagemaker_session.upload_data(path=\"./housing_data/raw/\", key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbd6d0-8202-46a7-8886-cd3ead2a793d",
   "metadata": {},
   "source": [
    "### 3. Define Parameters to Parametrize Pipeline Execution <a name=\"params\"></a><a name=\"params\"></a>\n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "- ParameterString - represents a str Python type\n",
    "- ParameterInteger - represents an int Python type\n",
    "- ParameterFloat - represents a float Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ee68a3-0d6b-4c72-bdad-9f159397b772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name='InputData', default_value=config_data['INPUT_DATA'])\n",
    "\n",
    "# status of newly trained model in registry\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=config_data['MODEL_APPROVAL_STATUS'])\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=config_data['PROCESSING_INSTANCE_TYPE']\n",
    ")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=config_data['PROCESSING_INSTANCE_COUNT'])\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=config_data['TRAINING_INSTANCE_TYPE'])\n",
    "\n",
    "# create model step parameters \n",
    "create_model_instance_type = ParameterString(name=\"CreateModelInstanceType\", default_value=config_data[\"CREATE_MODEL_INSTANCE_TYPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048766e-63fc-4147-a5b9-99269c49a5f1",
   "metadata": {},
   "source": [
    "### 4. Feature Build and Inference Script<a name=\"features\"></a><a name=\"features\"></a>\n",
    "\n",
    "Define a Sagemaker processing job for feature engineering, utilizing a scikit-learn StandardScaler(). Save the scaler as a feature artifact during training, and deserialize it for custom inference transformations.\n",
    "\n",
    "#### Script Structure\n",
    "\n",
    "Inside the main guard (`if name == __main__`), provide training code with arguments aligned to Sagemaker Processing Job documentation.\n",
    "\n",
    "Outside the main guard, define four inference functions as expected by Sagemaker:\n",
    "* `input_fn`: reads input data from the relative directory passed into the feature container\n",
    "* `model_fn`: deserializes the tar.gz artifact from the model registry, containing pretrained feature artifact(s)\n",
    "* `predict_fn`: computes the data transformation step for inference data\n",
    "* `output_fn`: sends transformed data to the model step container as JSON\n",
    "\n",
    "Refer to the Sagemaker Python SDK documentation for details. If no custom inference functions are provided, the default Sagemaker inference handler will run.\n",
    "\n",
    "`features.py` is the entry point for data preprocessing functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa10dee2-3a48-4773-9b75-fc5fab2c3a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8922452d-8118-4cfe-a93a-8a9d78d90c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/features.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile code/features.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tarfile\n",
    "\n",
    "try:\n",
    "    from sagemaker_containers.beta.framework import (\n",
    "        content_types,\n",
    "        encoders,\n",
    "        env,\n",
    "        modules,\n",
    "        transformer,\n",
    "        worker,\n",
    "        server,\n",
    "    )\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "feature_columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "]\n",
    "label_column = \"medianHouseValue\"\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "base_output_dir = \"/opt/ml/output/\"\n",
    "\n",
    "# feature build logic \n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(f\"{base_dir}/input/raw_data_all.csv\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[feature_columns], df[label_column], test_size=0.33)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train.values) # fit scaler without the feature names\n",
    "    x_train[feature_columns] = scaler.transform(x_train) \n",
    "\n",
    "    train_dataset = pd.concat([x_train, y_train], axis=1) \n",
    "    test_dataset = pd.concat([x_test, y_test], axis=1)\n",
    "    \n",
    "    train_dataset.to_csv(f\"{base_dir}/train/train.csv\", header=None, index=None) \n",
    "    test_dataset.to_csv(f\"{base_dir}/test/test.csv\", header=None, index=None)\n",
    "    \n",
    "    # save feature artifact for inference\n",
    "    joblib.dump(scaler, \"model.joblib\")\n",
    "    with tarfile.open(f\"{base_dir}/scaler_model/model.tar.gz\", \"w:gz\") as tar_handle:\n",
    "        tar_handle.add(f\"model.joblib\")\n",
    "\n",
    "# inference functions\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "    \"\"\"\n",
    "    print(\"Entering preprocessing input fn.\")\n",
    "    if content_type == \"text/csv\":\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), header=None) \n",
    "        \n",
    "        # If labelled, drop before inference\n",
    "        if len(df.columns) == len(feature_columns) + 1:\n",
    "            df.columns = feature_columns + [label_column]\n",
    "            df=df.drop(columns = label_column)\n",
    "            \n",
    "        # If unlabelled, continue\n",
    "        elif len(df.columns) == len(feature_columns):\n",
    "            df.columns = feature_columns\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "        \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\"\"\"\n",
    "    print(\"Entering preprocessing model fn.\")\n",
    "    preprocessor = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return preprocessor\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Apply feature transform to data\n",
    "    \"\"\"\n",
    "    print(\"Entering preprocessing predict fn.\")\n",
    "    features = model.transform(input_data.values) \n",
    "    return features\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    \"\"\"\n",
    "    print(\"Entering preprocessing output fn.\")\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append(row)\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == \"text/csv\":\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e3f9b-dbf9-4e0d-a497-2ed8044b0bb0",
   "metadata": {},
   "source": [
    "Defining preprocessing pipeline step parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e00bc1-3f49-454a-a577-df2063b2b02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:270: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn_framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls=SKLearn,\n",
    "    framework_version=sklearn_framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"pytorch-housing-data-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session, \n",
    "    code_location=f\"s3://{bucket}/{prefix}/processing\"\n",
    ")\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"scaler_model\", source=\"/opt/ml/processing/scaler_model\", destination = f\"s3://{bucket}/{prefix}/processing\"), \n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination = f\"s3://{bucket}/{prefix}/train\"), \n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination = f\"s3://{bucket}/{prefix}/test\"), \n",
    "    ],\n",
    "    code=\"code/features.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c75ef-f3ab-4e0a-81f4-b555499b5428",
   "metadata": {},
   "source": [
    "Wrap feature script in a Sagemaker Pipelines ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdc6e83-c8ac-44d6-be21-d131b5b7ab44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    step_args=processor_args,\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0046fa-6fa1-4654-b2f9-6c110ccf60da",
   "metadata": {},
   "source": [
    "### 5. Model Training and Inference Script <a name=\"model\"></a><a name=\"model\"></a>\n",
    "\n",
    "Demonstrates PyTorch model training and artifact registration. In custom inference functions, deserialize the model for prediction computation.\n",
    "\n",
    "#### Script Structure\n",
    "\n",
    "Inside the main guard (`if name == __main__`), provide training code with arguments for Sagemaker Training Job.\n",
    "\n",
    "Outside the main guard, define Sagemaker's expected inference functions:\n",
    "* `input_fn`: reads preprocessed input data from the feature step via the relative directory passed into the model step container\n",
    "* `model_fn`: deserializes the tar.gz artifact from the model registry containing any pretrained model artifacts\n",
    "* `predict_fn`: computes model inference predictions\n",
    "* (optional) `output_fn`: can configure additional custom handling of output predictions\n",
    "  \n",
    "Refer to the Sagemaker Python SDK documentation for model-specific inferencing details. If no custom inferencing functions are provided, the default Sagemaker inference handler will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d374ed35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/model.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class HousingDataset:\n",
    "    def __init__(self, train_dir):\n",
    "        self.train_dir = train_dir\n",
    "\n",
    "    def load_data(self):\n",
    "        train_dataset = pd.read_csv(os.path.join(self.train_dir, \"train.csv\"))\n",
    "\n",
    "        np_inputs = train_dataset.iloc[:, :-1].to_numpy()\n",
    "        inputs = torch.tensor(np_inputs).float()\n",
    "\n",
    "        np_targets = train_dataset.iloc[:, -1].to_numpy()\n",
    "        targets = torch.tensor(np_targets).float()\n",
    "\n",
    "        print(\"x train shape:\", np_inputs.shape, \"y train shape:\", np_targets.shape)\n",
    "\n",
    "        return TensorDataset(inputs, targets), np_inputs.shape, np_targets.shape\n",
    "\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, train_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                pred = self.model(xb)\n",
    "                loss = self.criterion(pred, yb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, loss.item()))\n",
    "\n",
    "    def save_model(self, model_dir):\n",
    "        path = os.path.join(model_dir, \"model.pth\")\n",
    "        torch.save(self.model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, input_object):\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for xb in input_object:\n",
    "                outputs = self.model(xb[0].to(self.device))\n",
    "                predictions.extend(outputs.tolist())\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--sm-model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_model(input_size, output_size, device):\n",
    "    model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "    return model\n",
    "\n",
    "# Model Training\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Passing in environment variables and hyperparameters for our training script\n",
    "    args, _ = parse_args()\n",
    "    print(\"Training data location: {}\".format(args.train))\n",
    "    \n",
    "    # Hyperparamaters\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "\n",
    "    print(\n",
    "        \"batch_size = {}, epochs = {}, learning rate = {}\".format(batch_size, epochs, learning_rate)\n",
    "    )\n",
    "    \n",
    "    # Reading in data\n",
    "    housing_dataset = HousingDataset(args.train)\n",
    "    train_dataset, input_shape, targets_shape = housing_dataset.load_data()\n",
    "    print(\"train dataset: \", train_dataset)\n",
    "    print(\"input shape: \", input_shape)\n",
    "    print(\"targets_shape: \", targets_shape)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Model Building\n",
    "    model = get_model(input_size=input_shape[1], output_size=1, device=device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    trainer = Trainer(model, criterion, optimizer, device)\n",
    "    trainer.train(train_loader, epochs)\n",
    "    \n",
    "    # save model artifact for inference\n",
    "    trainer.save_model(args.sm_model_dir)\n",
    "    \n",
    "# inference functions\n",
    "def reformat_inference_data(test_dataset):\n",
    "    \"\"\"Apply tensor structure and DataLoaders\n",
    "    \"\"\"\n",
    "    inputs = torch.tensor(test_dataset).float()\n",
    "    reformatted_inf_dataset = TensorDataset(inputs)\n",
    "    reformatted_inf_dataloader = DataLoader(reformatted_inf_dataset, batch_size=64)\n",
    "    return reformatted_inf_dataloader\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "    \"\"\"\n",
    "    print(\"Entering model input_fn.\")\n",
    "    if request_content_type == \"application/json\":\n",
    "        request_body = json.loads(request_body)\n",
    "        inpVar = request_body[\"instances\"]\n",
    "        inference_data = reformat_inference_data(inpVar)\n",
    "        return inference_data\n",
    "    else:\n",
    "        raise ValueError(\"This model only supports application/json input\")\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(8, 1, device)\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    \"\"\"Apply feature transform to data\n",
    "    \"\"\"\n",
    "    print(\"Entering predict_fn\")\n",
    "    predictor = Predictor(model, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return predictor.predict(input_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87b1cbe-59ef-4103-9712-91f2a33447e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "pytorch_framework_version = \"1.8.0\"\n",
    "model_path = f\"s3://{bucket}/{prefix}/pytorchmodel/\"\n",
    "\n",
    "training_pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=pytorch_framework_version,\n",
    "    py_version=\"py3\",\n",
    "    image_scope='training',\n",
    "    instance_type=config_data[\"TRAINING_INSTANCE_TYPE\"],\n",
    "    \n",
    ")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    sagemaker_session=pipeline_session,\n",
    "    entry_point=\"code/model.py\",\n",
    "    role=role,\n",
    "    output_path=model_path,\n",
    "    py_version=\"py3\",\n",
    "    image_uri=training_pytorch_image_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=config_data[\"TRAINING_INSTANCE_TYPE\"],\n",
    "    hyperparameters={\"epochs\": 5},\n",
    ")\n",
    "\n",
    "\n",
    "train_args = estimator.fit(\n",
    "    {\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "step_train_model = TrainingStep(name=\"TrainPyTorchModel\", step_args=train_args, cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c336f-6179-42f1-afb1-652fcc8291e6",
   "metadata": {},
   "source": [
    "### 6. Define a model creation step <a name=\"createregister\"></a><a name=\"createregister\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd43e78f-c7ec-451b-b037-d70e35b90c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import PipelineModel\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "\n",
    "scaler_model_s3 = \"{}/model.tar.gz\".format(\n",
    "    step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    ")\n",
    "\n",
    "scaler_model = SKLearnModel(\n",
    "    model_data=scaler_model_s3,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    entry_point=\"code/features.py\",\n",
    "    framework_version=sklearn_framework_version,\n",
    ")\n",
    "\n",
    "inference_pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=pytorch_framework_version,\n",
    "    py_version=\"py3\",\n",
    "    image_scope='inference',\n",
    "    instance_type=config_data[\"BATCH_TRANSFORM_INSTANCE_TYPE\"],\n",
    ")\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri = inference_pytorch_image_uri,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    entry_point=\"model.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    models=[scaler_model, pytorch_model], role=role, sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07234c95-5744-430c-b26e-b9388884c223",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"PyTorchPipelineModelCreation\",\n",
    "    step_args=pipeline_model.create(instance_type=create_model_instance_type),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66cefd-f59b-4c09-b8a2-47bb0935a355",
   "metadata": {},
   "source": [
    "### Define a Model Registration Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22263a5c-8e8a-40e3-80b3-17afb1ce7376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_args = pipeline_model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[processing_instance_type, training_instance_type],\n",
    "    transform_instances=[config_data[\"BATCH_TRANSFORM_INSTANCE_TYPE\"]],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    ")\n",
    "\n",
    "step_register_pipeline_model = ModelStep(\n",
    "    name=\"PyTorchPipelineModelRegistration\",\n",
    "    step_args=register_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec21c5b-553b-4165-aca6-e51562fd06ed",
   "metadata": {},
   "source": [
    "### 7. Define a Sagemaker Pipeline<a name=\"pipeline\"></a><a name=\"pipeline\"></a>\n",
    "\n",
    "Wrap the feature building and model building for training and inference in a Sagemaker Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2d075ce-5972-4c5b-bdeb-80bbb130beb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    parameters=[\n",
    "        training_instance_type,\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        input_data,\n",
    "        model_approval_status,\n",
    "    ],\n",
    "    \n",
    "    steps = [step_process, step_train_model, step_create_model, step_register_pipeline_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacf8ff-8e2d-401b-91cb-adacf9a90bce",
   "metadata": {},
   "source": [
    "Display Pipeline definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73883245-c7ef-4cd9-8c10-77916fe30ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/serial-pytorch-pipeline/code/bf5f270b1960b4061b98c1625e4aa582/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://pytorch-serial-inference-demo/serial-pytorch-pipeline/code/ece5752bed68960c30544c25e83cd9ee/runproc.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "Using provided s3_resource\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/housing_data/raw'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'Approved'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'PreprocessData',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['/bin/bash',\n",
       "      '/opt/ml/processing/input/entrypoint/runproc.sh']},\n",
       "    'RoleArn': 'arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/serial-pytorch-pipeline/code/bf5f270b1960b4061b98c1625e4aa582/sourcedir.tar.gz',\n",
       "       'LocalPath': '/opt/ml/processing/input/code/',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'entrypoint',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://pytorch-serial-inference-demo/serial-pytorch-pipeline/code/ece5752bed68960c30544c25e83cd9ee/runproc.sh',\n",
       "       'LocalPath': '/opt/ml/processing/input/entrypoint',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'scaler_model',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing',\n",
       "        'LocalPath': '/opt/ml/processing/scaler_model',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': '30d'}},\n",
       "  {'Name': 'TrainPyTorchModel',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.ca-central-1.amazonaws.com/pytorch-training:1.8.0-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/pytorchmodel/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.PreprocessData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'}],\n",
       "    'HyperParameters': {'epochs': '5',\n",
       "     'sagemaker_submit_directory': '\"s3://pytorch-serial-inference-demo/serial-pytorch-pipeline/code/8409fd8d074d119391b85972373180b4/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"model.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"ca-central-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/pytorchmodel/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/pytorchmodel/',\n",
       "     'DisableProfiler': False}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': '30d'}},\n",
       "  {'Name': 'PyTorchPipelineModelCreation-RepackModel-1',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pytorch-inference-2023-12-15-21-29-09-015'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.large'},\n",
       "    'RoleArn': 'arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': 'Steps.TrainPyTorchModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'training'}],\n",
       "    'HyperParameters': {'inference_script': '\"model.py\"',\n",
       "     'model_archive': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Steps.TrainPyTorchModel.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "     'dependencies': 'null',\n",
       "     'source_dir': '\"code\"',\n",
       "     'sagemaker_submit_directory': '\"s3://pytorch-serial-inference-demo/PyTorchPipelineModelCreation-RepackModel-1-89cc02254af4b707d205a66513aaf552/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"ca-central-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pytorch-inference-2023-12-15-21-29-09-015',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'DisableProfiler': True}},\n",
       "   'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "  {'Name': 'PyTorchPipelineModelCreation-CreateModel',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063',\n",
       "    'Containers': [{'Image': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "      'Environment': {'SAGEMAKER_PROGRAM': 'features.py',\n",
       "       'SAGEMAKER_SUBMIT_DIRECTORY': 's3://pytorch-serial-inference-demo/sagemaker-scikit-learn-2023-12-15-21-29-08-895/sourcedir.tar.gz',\n",
       "       'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "       'SAGEMAKER_REGION': 'ca-central-1'},\n",
       "      'ModelDataUrl': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/model.tar.gz'},\n",
       "     {'Image': '763104351884.dkr.ecr.ca-central-1.amazonaws.com/pytorch-inference:1.8.0-cpu-py3',\n",
       "      'Environment': {'SAGEMAKER_PROGRAM': 'model.py',\n",
       "       'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "       'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "       'SAGEMAKER_REGION': 'ca-central-1'},\n",
       "      'ModelDataUrl': {'Get': 'Steps.PyTorchPipelineModelCreation-RepackModel-1.ModelArtifacts.S3ModelArtifacts'}}]}},\n",
       "  {'Name': 'PyTorchPipelineModelRegistration-RepackModel-1',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pytorch-inference-2023-12-15-21-29-09-321'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.large'},\n",
       "    'RoleArn': 'arn:aws:iam::817463428454:role/service-role/AmazonSageMaker-ExecutionRole-20230919T125063',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': 'Steps.TrainPyTorchModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'training'}],\n",
       "    'HyperParameters': {'inference_script': '\"model.py\"',\n",
       "     'model_archive': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Steps.TrainPyTorchModel.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "     'dependencies': 'null',\n",
       "     'source_dir': '\"code\"',\n",
       "     'sagemaker_submit_directory': '\"s3://pytorch-serial-inference-demo/PyTorchPipelineModelRegistration-RepackModel-1-89cc02254af4b707d205a66513aaf552/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"ca-central-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://pytorch-serial-inference-demo/pytorch-inference-2023-12-15-21-29-09-321',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'DisableProfiler': True}},\n",
       "   'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "  {'Name': 'PyTorchPipelineModelRegistration-RegisterModel',\n",
       "   'Type': 'RegisterModel',\n",
       "   'Arguments': {'ModelPackageGroupName': 'PipelinePyTorchPackageGroup',\n",
       "    'InferenceSpecification': {'Containers': [{'Image': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "       'Environment': {'SAGEMAKER_PROGRAM': 'features.py',\n",
       "        'SAGEMAKER_SUBMIT_DIRECTORY': 's3://pytorch-serial-inference-demo/sagemaker-scikit-learn-2023-12-15-21-29-09-220/sourcedir.tar.gz',\n",
       "        'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "        'SAGEMAKER_REGION': 'ca-central-1'},\n",
       "       'ModelDataUrl': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/model.tar.gz'},\n",
       "      {'Image': '763104351884.dkr.ecr.ca-central-1.amazonaws.com/pytorch-inference:1.8.0-cpu-py3',\n",
       "       'Environment': {'SAGEMAKER_PROGRAM': 'model.py',\n",
       "        'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "        'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "        'SAGEMAKER_REGION': 'ca-central-1'},\n",
       "       'ModelDataUrl': {'Get': 'Steps.PyTorchPipelineModelRegistration-RepackModel-1.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "     'SupportedContentTypes': ['text/csv'],\n",
       "     'SupportedResponseMIMETypes': ['text/csv'],\n",
       "     'SupportedRealtimeInferenceInstanceTypes': [{'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      {'Get': 'Parameters.TrainingInstanceType'}],\n",
       "     'SupportedTransformInstanceTypes': ['ml.m4.xlarge']},\n",
       "    'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}}}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46944c33-68c6-4439-90b5-34720896d453",
   "metadata": {},
   "source": [
    "### 8. Submit the pipeline and start execution <a name=\"submit\"></a><a name=\"submit\"></a>\n",
    "\n",
    "Running steps to upsert the `role_arn` and start the [pipeline execution](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html) will kick-off training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de509700-7306-4e8f-986a-56babc0846d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/serial-pytorch-pipeline/code/bf5f270b1960b4061b98c1625e4aa582/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://pytorch-serial-inference-demo/serial-pytorch-pipeline/code/ece5752bed68960c30544c25e83cd9ee/runproc.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "Using provided s3_resource\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/serial-pytorch-pipeline/code/bf5f270b1960b4061b98c1625e4aa582/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://pytorch-serial-inference-demo/serial-pytorch-pipeline/code/ece5752bed68960c30544c25e83cd9ee/runproc.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "Using provided s3_resource\n",
      "Using provided s3_resource\n",
      "Using provided s3_resource\n",
      "------- done -------\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.wait()\n",
    "print(\"------- done -------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bdf39-9b31-485a-8d34-5b45a906cea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Get name of the latest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e7c575d-2cd0-4a25-9e67-658c1b16e9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipelines-efdkwygjfuvn-PyTorchPipelineModel-kLvjP2vy25\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = sm.list_models(NameContains='PyTorch')['Models'][0]['ModelName']\n",
    "\n",
    "print(sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c39587-7bbd-4444-915d-568aede846b7",
   "metadata": {},
   "source": [
    "List model registry information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3316c5f8-5fc4-4cc6-ae28-fe00b74b2739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Identified the latest approved model package: arn:aws:sagemaker:ca-central-1:817463428454:model-package/PipelinePyTorchPackageGroup/26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelPackageGroupName': 'PipelinePyTorchPackageGroup', 'ModelPackageVersion': 26, 'ModelPackageArn': 'arn:aws:sagemaker:ca-central-1:817463428454:model-package/PipelinePyTorchPackageGroup/26', 'CreationTime': datetime.datetime(2023, 12, 15, 21, 31, 58, 964000, tzinfo=tzlocal()), 'InferenceSpecification': {'Containers': [{'Image': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3', 'ImageDigest': 'sha256:9b43ef4706faae38d10bdff012a0d1b35ed9c5b3aac9e60c960170f10d29fa51', 'ModelDataUrl': 's3://pytorch-serial-inference-demo/pipeline-pytorch-example/processing/model.tar.gz', 'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20', 'SAGEMAKER_PROGRAM': 'features.py', 'SAGEMAKER_REGION': 'ca-central-1', 'SAGEMAKER_SUBMIT_DIRECTORY': 's3://pytorch-serial-inference-demo/sagemaker-scikit-learn-2023-12-15-21-29-09-220/sourcedir.tar.gz'}}, {'Image': '763104351884.dkr.ecr.ca-central-1.amazonaws.com/pytorch-inference:1.8.0-cpu-py3', 'ImageDigest': 'sha256:7c4c7ea4f0e8cfe25441880fe23ffcac5997c03e00358e4e22567eb160f8537d', 'ModelDataUrl': 's3://pytorch-serial-inference-demo/pytorch-inference-2023-12-15-21-29-09-321/pipelines-efdkwygjfuvn-PyTorchPipelineModel-pSPsUELHLN/output/model.tar.gz', 'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20', 'SAGEMAKER_PROGRAM': 'model.py', 'SAGEMAKER_REGION': 'ca-central-1', 'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'}}], 'SupportedTransformInstanceTypes': ['ml.m4.xlarge'], 'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.xlarge', 'ml.m5.xlarge'], 'SupportedContentTypes': ['text/csv'], 'SupportedResponseMIMETypes': ['text/csv']}, 'ModelPackageStatus': 'Completed', 'ModelPackageStatusDetails': {'ValidationStatuses': [], 'ImageScanStatuses': []}, 'CertifyForMarketplace': False, 'ModelApprovalStatus': 'Approved', 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::817463428454:assumed-role/AmazonSageMaker-ExecutionRole-20230919T125063/sagemaker-pipeline-efdkwygjfuvn-PyTorchPipelineModel', 'PrincipalId': 'AROA34VE6AVTANXNYKS57:sagemaker-pipeline-efdkwygjfuvn-PyTorchPipelineModel'}}, 'MetadataProperties': {'GeneratedBy': 'arn:aws:sagemaker:ca-central-1:817463428454:pipeline/serial-pytorch-pipeline/execution/efdkwygjfuvn'}, 'ResponseMetadata': {'RequestId': 'aad1b4eb-ca36-4e98-ba1b-0606b5a99577', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'aad1b4eb-ca36-4e98-ba1b-0606b5a99577', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2100', 'date': 'Fri, 15 Dec 2023 21:33:33 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from utils import get_approved_package\n",
    "\n",
    "pck = get_approved_package(model_package_group_name, sm) \n",
    "model_description = sm.describe_model_package(ModelPackageName=pck[\"ModelPackageArn\"])\n",
    "\n",
    "print(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733c17a-2196-473d-a539-a934317e037f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Run serial batch inference job <a name=\"inference\"></a>\n",
    "After the pipeline has finished executing, lookup the Sagemaker Model Name and pass it to a Sagemaker Batch Transformation job for inference, along with a raw test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54e7447b-9679-45f0-9f10-e00f9fba623b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: inference-pipelines-batch-2023-12-15-21-33-34-489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................\u001b[34m2023-12-15 21:40:45,105 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,108 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,109 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,105 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,108 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,109 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Module features does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Module features does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2023-12-15 21:40:45,406 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: features\n",
      "  Building wheel for features (setup.py): started\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: features\n",
      "  Building wheel for features (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for features (setup.py): finished with status 'done'\n",
      "  Created wheel for features: filename=features-1.0.0-py2.py3-none-any.whl size=5232 sha256=ec64bdbc0a8774b7c4474c36b46053be5ceb5d44de6fefa4151d3ae362f64e39\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-fkon05iw/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built features\u001b[0m\n",
      "\u001b[34mInstalling collected packages: features\u001b[0m\n",
      "\u001b[34mSuccessfully installed features-1.0.0\u001b[0m\n",
      "\u001b[35m  Building wheel for features (setup.py): finished with status 'done'\n",
      "  Created wheel for features: filename=features-1.0.0-py2.py3-none-any.whl size=5232 sha256=ec64bdbc0a8774b7c4474c36b46053be5ceb5d44de6fefa4151d3ae362f64e39\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-fkon05iw/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[35mSuccessfully built features\u001b[0m\n",
      "\u001b[35mInstalling collected packages: features\u001b[0m\n",
      "\u001b[35mSuccessfully installed features-1.0.0\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [33] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [33] [INFO] Listening at: unix:/tmp/gunicorn.sock (33)\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [33] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2023-12-15 21:40:49 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [33] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [33] [INFO] Listening at: unix:/tmp/gunicorn.sock (33)\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [33] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[35m[2023-12-15 21:40:49 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,184 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[32mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[32mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[32mCurrent directory: /\u001b[0m\n",
      "\u001b[32mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[32mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[32mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[32mMax heap size: 2838 M\u001b[0m\n",
      "\u001b[32mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[32mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[32mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[32mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[32mInitial Models: model.mar\u001b[0m\n",
      "\u001b[32mLog dir: /logs\u001b[0m\n",
      "\u001b[32mMetrics dir: /logs\u001b[0m\n",
      "\u001b[32mNetty threads: 0\u001b[0m\n",
      "\u001b[32mNetty client threads: 0\u001b[0m\n",
      "\u001b[32mDefault workers per model: 4\u001b[0m\n",
      "\u001b[32mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[32mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[32mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[32mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[32mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[32mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[32mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[32mEnable metrics API: true\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,230 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,246 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 86ce9a0dad2c42359110bd39c7ad2b06\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,263 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,312 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,532 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,540 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]45\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,541 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,542 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,555 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,691 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,691 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,693 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]47\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]48\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,695 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,694 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,735 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,735 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,752 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,772 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,774 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,785 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,813 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,822 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]46\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,823 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,823 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,823 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:46,903 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[32mModel server started.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,736 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,740 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:42.70233917236328|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,741 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:13.162792205810547|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,742 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:23.6|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,743 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14718.8828125|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,744 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:980.01953125|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:47,745 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:8.2|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676447\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:49,574 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:49,584 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:49,594 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:49,596 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,596 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4630\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,597 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:5323|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676451\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,598 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:65|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,740 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4806\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,741 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:5463|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676451\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,742 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:145|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,899 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4986\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,901 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:5623|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676451\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,901 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:125|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,939 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5024\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,939 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:5661|#Level:Host|#hostname:15f8a21d45c7,timestamp:1702676451\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:51,940 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:125|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:56,763 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:33326 \"GET /ping HTTP/1.1\" 200 12\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:56,764 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:57,396 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:33342 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:57,396 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[34mEntering preprocessing model fn.\u001b[0m\n",
      "\u001b[34mEntering preprocessing input fn.\u001b[0m\n",
      "\u001b[34mEntering preprocessing predict fn.\u001b[0m\n",
      "\u001b[34mEntering preprocessing output fn.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [15/Dec/2023:21:40:58 +0000] \"POST /invocations HTTP/1.1\" 200 1142387 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mEntering preprocessing model fn.\u001b[0m\n",
      "\u001b[35mEntering preprocessing input fn.\u001b[0m\n",
      "\u001b[35mEntering preprocessing predict fn.\u001b[0m\n",
      "\u001b[35mEntering preprocessing output fn.\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [15/Dec/2023:21:40:58 +0000] \"POST /invocations HTTP/1.1\" 200 1142387 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,313 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering model input_fn.\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,314 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering predict_fn\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,314 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:102.92|#ModelName:model,Level:Model|#hostname:15f8a21d45c7,requestID:40b91428-8842-45d2-9541-ab6de96867d6,timestamp:1702676458\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,316 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 106\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,316 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:33352 \"POST /invocations HTTP/1.1\" 200 120\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,316 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,317 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[32m2023-12-15 21:40:58,317 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:15f8a21d45c7,timestamp:null\u001b[0m\n",
      "\u001b[36m2023-12-15T21:40:57.402:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "input_data_path = \"s3://{}/{}\".format(sagemaker_session.default_bucket(), f\"{prefix}/test/test.csv\") \n",
    "output_data_path = \"s3://{}/{}\".format(sagemaker_session.default_bucket(), f\"{prefix}/batch-transform/output\")\n",
    "\n",
    "transform_job = sagemaker.transformer.Transformer(\n",
    "    model_name = sm_model_name,\n",
    "    instance_count = config_data[\"BATCH_TRANSFORM_INSTANCE_COUNT\"],\n",
    "    instance_type = config_data[\"BATCH_TRANSFORM_INSTANCE_TYPE\"],\n",
    "    strategy = 'MultiRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='inference-pipelines-batch',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept = 'text/csv') \n",
    "\n",
    "transform_job.transform(data = input_data_path, \n",
    "                        content_type = 'text/csv', \n",
    "                        split_type = 'Line',\n",
    "                        join_source = 'Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26182734-3ce2-4935-ba6a-2bedd097cf6f",
   "metadata": {},
   "source": [
    "### 10. Retrieve output batch inference data<a name=\"batch_inference\"></a>\n",
    "\n",
    "After the batch transform job has completed, you can download and read inference predictions for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f304219-07e0-41dd-bbb0-42db18126269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "FILE_NAME = \"inference_results.csv\"\n",
    "BUCKET_NAME = bucket\n",
    "OBJECT_NAME = f\"{prefix}/batch-transform/output/test.csv.out\"\n",
    "\n",
    "s3.download_file(BUCKET_NAME, OBJECT_NAME, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16a4644e-4426-4878-b7ee-68779cbf2fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housingMedianAge</th>\n",
       "      <th>totalRooms</th>\n",
       "      <th>totalBedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>medianIncome</th>\n",
       "      <th>medianHouseValue</th>\n",
       "      <th>predictedTarget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.418929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-118.36</td>\n",
       "      <td>34.19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2921.0</td>\n",
       "      <td>685.0</td>\n",
       "      <td>1512.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>4.1445</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.383919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.39</td>\n",
       "      <td>37.60</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>4.6520</td>\n",
       "      <td>0.774200</td>\n",
       "      <td>0.423604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-121.30</td>\n",
       "      <td>38.60</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9534.0</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>4951.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>3.3926</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.406280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.83</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>7.8772</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>0.428225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>-118.48</td>\n",
       "      <td>34.02</td>\n",
       "      <td>11.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.6250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.385648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6808</th>\n",
       "      <td>-117.39</td>\n",
       "      <td>34.10</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7184.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>4862.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>2.4492</td>\n",
       "      <td>0.207600</td>\n",
       "      <td>0.367412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6809</th>\n",
       "      <td>-121.38</td>\n",
       "      <td>38.59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>685.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>4.5313</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.421584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6810</th>\n",
       "      <td>-118.43</td>\n",
       "      <td>34.22</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>3.6618</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.385341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6811</th>\n",
       "      <td>-118.18</td>\n",
       "      <td>33.77</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>2.6371</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.380935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6812 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      longitude  latitude  housingMedianAge  totalRooms  totalBedrooms   \n",
       "0       -122.27     37.84              52.0      2436.0          541.0  \\\n",
       "1       -118.36     34.19              11.0      2921.0          685.0   \n",
       "2       -122.39     37.60              44.0      2304.0          384.0   \n",
       "3       -121.30     38.60              32.0      9534.0         1819.0   \n",
       "4       -122.23     37.83              52.0      2990.0          379.0   \n",
       "...         ...       ...               ...         ...            ...   \n",
       "6807    -118.48     34.02              11.0        72.0           16.0   \n",
       "6808    -117.39     34.10              12.0      7184.0         1516.0   \n",
       "6809    -121.38     38.59              38.0      1839.0          287.0   \n",
       "6810    -118.43     34.22              36.0      1372.0          295.0   \n",
       "6811    -118.18     33.77              30.0      1418.0          439.0   \n",
       "\n",
       "      population  households  medianIncome  medianHouseValue  predictedTarget  \n",
       "0         1015.0       478.0        1.7250          0.227800         0.418929  \n",
       "1         1512.0       664.0        4.1445          0.352800         0.383919  \n",
       "2          986.0       379.0        4.6520          0.774200         0.423604  \n",
       "3         4951.0      1710.0        3.3926          0.206800         0.406280  \n",
       "4          947.0       361.0        7.8772          1.000002         0.428225  \n",
       "...          ...         ...           ...               ...              ...  \n",
       "6807       150.0        20.0        2.6250          0.500000         0.385648  \n",
       "6808      4862.0      1235.0        2.4492          0.207600         0.367412  \n",
       "6809       685.0       276.0        4.5313          0.378800         0.421584  \n",
       "6810       774.0       306.0        3.6618          0.374600         0.385341  \n",
       "6811       720.0       417.0        2.6371          0.318800         0.380935  \n",
       "\n",
       "[6812 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "    \"predictedTarget\"\n",
    "]\n",
    "\n",
    "inf_results = pd.read_csv(\"inference_results.csv\", names=columns)\n",
    "\n",
    "display(inf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db58fdd-a7b7-41b2-9719-a5395ebdde51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ca-central-1:310906938811:image/pytorch-2.0.0-cpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
